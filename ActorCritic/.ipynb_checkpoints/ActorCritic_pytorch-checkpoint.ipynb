{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code implement the Actor Critic Alogrithms\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "LEARNING_RATE_ACTOR = 0.001\n",
    "LEARNING_RATE_CRITIC = 0.01\n",
    "GAMMA = 0.9\n",
    "OUTPUT_GRAPH = False\n",
    "ENVNAME = \"CartPole-v0\"\n",
    "NUM_EPISODES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4325, 0.5675], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s = torch.Tensor(env.observation_space.sample())\n",
    "s = torch.from_numpy(env.reset()).float()\n",
    "model = Actor_Network(4,2)\n",
    "model.forward(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1920928955078125e-07"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Network(nn.Module):\n",
    "    def __init__(self, n_features, n_action):\n",
    "        super(Actor_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 20)\n",
    "        self.fc2 = nn.Linear(20, n_action)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x),dim=-1)\n",
    "        return x     \n",
    "    \n",
    "class Critic_Network(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Critic_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 20)\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, n_features, n_actions, lr=0.001):\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = lr\n",
    "        self.model = Actor_Network(self.n_features, self.n_actions)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        \n",
    "    def learn(self, s, a, td):\n",
    "        tensor_states = torch.from_numpy(s).float()\n",
    "        td_error = torch.Tensor(td)\n",
    "    \n",
    "        action_prob = self.model.forward(tensor_states)\n",
    "        log_prob = torch.log(action_prob[a])\n",
    "        exp_v = torch.mean(log_prob * td_error)\n",
    "        \n",
    "        self.loss_actor = -exp_v\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss_actor.backward()\n",
    "        self.optimizer.step()\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = torch.from_numpy(observation).float()\n",
    "        prob_weights = self.model.forward(state)\n",
    "        action_ = torch.distributions.Categorical(prob_weights)\n",
    "        action = action_.sample()\n",
    "        return action.item()\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, n_features, lr=0.01):\n",
    "        self.n_features = n_features\n",
    "        self.lr = lr\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model =Critic_Network(self.n_features)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        tensor_states = torch.from_numpy(s).float()\n",
    "        tensor_states_ = torch.from_numpy(s_).float()\n",
    "        tensor_reward = torch.Tensor(np.array(r))\n",
    "        self.v =self.model.forward(tensor_states)\n",
    "        self.v_ =self.model.forward(tensor_states_)\n",
    "        self.td_error = tensor_reward + GAMMA * self.v_ - self.v\n",
    "        self.loss_critic = self.td_error ** 2\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss_critic.backward()\n",
    "        self.optimizer.step()        \n",
    "        \n",
    "        return self.td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_stats1(rec, xlabel, ylabel,title):\n",
    "    fig = plt.figure(figsize=(20, 10), facecolor = \"white\")\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(rec) \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    return fig\n",
    "\n",
    "def plot_episode_stats2(stats):\n",
    "    # Plot time steps and episode number\n",
    "    fig3 = plt.figure(figsize=(20, 10))\n",
    "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Episode\")\n",
    "    plt.title(\"Episode per time step\")\n",
    "    return fig3\n",
    "\n",
    "def Plot_the_result(rec):\n",
    "    # Plot episode length over time\n",
    "    episode_lengths = rec.episode_lengths\n",
    "    fig = plot_episode_stats1(episode_lengths, \n",
    "                       xlabel = \"Episode\",\n",
    "                       ylabel = \"Episode Length\",\n",
    "                       title = \"Episode length over Time\"\n",
    "            )\n",
    "    fig.show()\n",
    "    fig.savefig(\"./log/AC_keras_CartPole_EpisodeLength.jpg\")\n",
    "\n",
    "    # Plot Episode reward over time\n",
    "    smoohing_window = 10\n",
    "    reward_smooths = pd.Series(rec.episode_rewards).rolling(smoohing_window,\\\n",
    "                    min_periods = smoohing_window).mean()\n",
    "    fig = plot_episode_stats1(reward_smooths, \n",
    "                       xlabel = \"Episode\",\n",
    "                       ylabel = \"Episode Reward\",\n",
    "                       title = \"Episode reward over time\"\n",
    "            )\n",
    "    fig.show()\n",
    "    fig.savefig(\"./log/AC_keras_CartPole_EpisodeReward.jpg\")\n",
    "    \n",
    "    # Plot Episode per time step\n",
    "    fig = plot_episode_stats2(rec)\n",
    "    fig.show()\n",
    "    fig.savefig(\"./log/AC_keras_CartPole_EpisodePerTimeStep.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(actor, critic, env, num_episodes):\n",
    "    # Track the statistics of the result\n",
    "    record = namedtuple(\"Record\", [\"episode_lengths\",\"episode_rewards\"])\n",
    "    \n",
    "    rec = record(episode_lengths=np.zeros(num_episodes),\n",
    "                          episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        if 0 == (i_episode +1) % 1:\n",
    "            print(\"This the episode {}/{}\".format(i_episode, num_episodes), end = \"\\r\")\n",
    "        observation = env.reset()\n",
    "        step =0\n",
    "        reward_cum = 0\n",
    "        done = False\n",
    "        while True:\n",
    "            #env.render()\n",
    "            if RENDER_ENV:\n",
    "                env.render()\n",
    "            action = actor.choose_action(observation)\n",
    "            observation_next, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                reward = -20\n",
    "            # update the record\n",
    "            step += 1\n",
    "            rec.episode_lengths[i_episode] = step \n",
    "            rec.episode_rewards[i_episode] += reward\n",
    "            td_error = critic.learn(observation, reward, observation_next)\n",
    "            actor.learn(observation, action, td_error)\n",
    "            if done:\n",
    "                if 0 == (i_episode +1) % 5:\n",
    "                    print(\"The reward at episode {} is {}.\".format(i_episode, \n",
    "                                            rec.episode_rewards[i_episode]))\n",
    "                break\n",
    "            observation = observation_next\n",
    "    print(\"Finished\")\n",
    "    env.close()\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This the episode 0/500\r",
      "----------- tensor(0.9022, grad_fn=<NegBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-f2a6b4d98f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE_CRITIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#Plot the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mPlot_the_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-093e630acc02>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(actor, critic, env, num_episodes)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi_episode\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-8cfaeece2713>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, s, a, td)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_actor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexp_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.envs.make(ENVNAME)\n",
    "    env = env.unwrapped\n",
    "    env.seed(1)\n",
    "    n_action = env.action_space.n\n",
    "    n_feature = env.observation_space.shape[0]\n",
    "    \n",
    "    actor = Actor(n_features=n_feature, n_actions=n_action, lr=LEARNING_RATE_ACTOR)\n",
    "    critic = Critic(n_features=n_feature, lr=LEARNING_RATE_CRITIC) \n",
    "\n",
    "    rec = update(actor, critic, env, num_episodes=NUM_EPISODES)\n",
    "    #Plot the result\n",
    "    Plot_the_result(rec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
