\documentclass{article}
\usepackage{iclr2018_conference,times}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\newcommand{\argmax}{\operatorname*{argmax}}
\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\var}{\operatorname*{Var}}
\newcommand{\lstd}{\operatorname*{LSTD}}
\newcommand{\diag}{\operatorname*{diag}}
\newcommand{\sgn}{\operatorname*{sgn}}
\newcommand{\lin}{\operatorname*{lin}}
\newcommand{\supp}{\operatorname*{Supp}}
\newcommand{\todo}[2]{{\color{yellow}{#1 TODO: #2}}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
%\input{publications.bbl}
\title{A survey on value-based deep reinforcement learning}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\author{Feng Xiaolong  \\
	\texttt{xlfeng886@163.com} \\
}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\iclrfinalcopy
\begin{document}
\maketitle
\input{symbolsdef.tex}


.
\begin{abstract}
Reinforcement learning (RL) is developed to address the problem of how to make a sequential decision. The goal of the RL algorithm is to maximize the total reward when the agent interaction with the enviroment. RL is very successful in many traditional fields for decades. From another aspect of AI technology,  deep learning (DL) has become very popular in recent years and  has extended the boundary of many tradition research filed. Combine with RL and DL is the fundamental idea for researchers recently. The first breakout come from the DQN algorithm, which is proposed by \citep{mnih2013playing}. We defined the new technology as deep reinforcement learning (DRL). DRL create a new branch in both the RL field and DL field. Since the DQN algorithm \citep{mnih2013playing}, there are many new algorithms have been proposed, and the number of related papers has grown exponentially in recent years. In this paper, we focus on the value-based deep reinforcement learning method, which is one of the basis of the DRL algorithm sets. Many papers are derived from the original DQN algorithm.
\end{abstract}
%
\section{Introduction}
\label{sec:Introduction}
We can consider the essential question in computer science, how to build an efficient artificial intelligence (AI)? Firstly, it must face the uncertain of the world and the complicated structure. Second, it should know how to make the sequence decision in the real world. Third, as \citep{sutton2018reinforcement} discussed the mechanism of how humans affected by the dopamine, the AI agent should also be derived by the reward and towards the maximum of the total reward during the learning progress. In recent years, DRL is the most popular method to approach the efficient AI system. DRL utilizes the DL method to deal with the unstructured of the world. 
In the past decades, the researchers hand-designed the relevant features when they deal with the specific problem. The  tradtional pipeline cost too much human resources but can not increase the performance too much. With the rapid development of DL technology and its widespread application in various fields, state-of-art in many areas has been continuously refreshed. The reinforcement learning field is also one of the fields that deeply affected by DL technology. As we know, RL is inspired by the behavioral psychology   \citep{sutton2018reinforcement}. It can handle the sequential decision-making problem. So, the combination of reinforcement learning and deep learning leads to a newborn deep reinforcement learning technology. The DRL technology fills us with visions of the future, and it has extended the boundaries of our cognition, allowing revolutionary changes in many applications.

We could find a lot of achievements brought by the DRL technology  from \citep{lecun2015deep, schmidhuber2015deep, goodfellow2016deep}. For example, \citep{mnih2015human} utilized the DRL agent to learn the raw pixels of the Atari game and achieve human-level performance. \citep{silver2016mastering} can defeat human champions in the go game. \citep{silver2018general} use DRL to play chess and Shogi by the self-play method. \citep{vinyals2019alphastar} applied DRL in complicated strategy games and achieve a fantastic result. DRL also performs very well in real-world applications, such as robotics \citep{gandhi2017learning, pinto2017asymmetric}, self-driving cars \citep{pan2017virtual}. We also can see some very successful application cases in the finance field \citep{deng2016deep}. 

Here we need to have an overview of the DRL algorithm taxonomy. As we can see, DRL is a fast-developing field, and it is not easy to draw the taxonomy accurately. We can have some traditional perspectives for this, which ignore the most advanced area of DRL (such as meta-learning, transfer learning, MAML exploration, and some other fields).

The root branching points in a DRL algorithm is to consider the question of whether we can access to the environment model. If the state transitions and rewards can be predicted from the simulated model, we can consider it as the model-based DRL algorithm. If we have no idea about the transitions and rewards, we should consider it as the model-free DRL algorithm.

For model-free DRL algorithms, we could have A2C algorithm, A3C algorithm\citep{mnih2016asynchronous}, PPO algorithm \citep{schulman2017proximal}, TRPO algorithm \citep{schulman2015trust}), Q-learning algorithm \citep{mnih2013playing}, C51 algorithm \citep{bellemare2017distributional}, QR-DQN algorithm \citep{dabney2018distributional}, ER algorithm \citep{andrychowicz2017hindsight}), TD3 algorithm \citep{fujimoto2018addressing}, SAC algorithm \citep{haarnoja2018soft}, etc.

For model-based DRL algorithm, it should consider whether its model is given or the model is unknown. Alpha Zero \citep{silver2018general} is the application of the given model. If the model is unknown, we could have some algorithms such as I2A, MBMF, MBVE. 

In this article, we only survey the value-based deep reinforcement learning. 
%
\section{Problem formulation}
\label{sec:Problem formulation}
Reinforcement Learning \citep{sutton2018reinforcement} is to learn how to make sequential decision when the AI agent interact with the environments $\mathbb{E}$. We will formalize the $\mathbb{E}$ as a Markov Decision Processes (MDPs). it could be described as the tuple $({S},{A},{P},{R})$.  That means in each time step, the AI agent interact with the observed state $s_t \in {S}$, and chooses the action $a_t \in {A}$ . The action will lead to the reward $r_t \sim {R}$ and next state $s_{t+1} \sim {P}$.

The purpose of the learning process is to maximize the total reward when the AI agent interacts with the environments $\mathbb{E}$. In the infinite case, we also consider the discount factor $\gamma$ per time-step. The total discounted return  at time step $t$ should be  $R_t = \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'}$, where $T$ is the time-step and we ignore the step before time step $t$ due to the causality law.

The optimal action-value function $Q^*(s,a)$  is  the maximum expected return on the condition below $s_t$ and $a_t$. We can define  it as $Q^*(s,a) = \max_{\pi} \mathbb{E}[{ R_t | s_t=s, a_t=a, \pi }]$, where $\pi$ is the policy. The $Q^*(s,a)$ function will follow the  Bellman equation and update the next $Q$ value iteratively as $Q_{i+1}(s,a) = \mathbb{E}\left[ r + \gamma \max_{a'} Q_i(s', a') | s, a \right]$. This is proved to converge to optimal $Q^ * $ in tabular case \citep{sutton2018reinforcement}. 
In the reinforcement learning community, this is typically a linear function approximator, but sometimes a non-linear function approximator is used instead, such as a neural network. When we use the neural network, we parameterize the $Q$-network according to the weight $\theta$ and train it by minimizing the mean square error (MSE) loss function. The parameter $\theta$ will change iteratively as below:

\begin{align}
	argmin _\theta  \ L_i\left(\theta_i\right) &={E}_{s,a \sim P(\cdot)}{\left({E}_{s' \sim {E}}[{r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) | s, a }] - Q \left(s,a ; \theta_i \right) \right)^2}
\end{align}


In the optimization progress, previous parameters  $\theta_{i-1}$ will be fixed.  The optimization method could be one of the most popular methods, such as ADAM, RMSprop, Momentum SGD, etc.
%

\section{Related work}
\label{sec:Related work}
For decades, scholars have made many outstanding contributions to the field of RL, and traditional RL algorithms are very mature.  We could find many papers that review the traditional RL, such as \citep{gosavi2009reinforcement, kaelbling1996reinforcement, sutton2018reinforcement, szepesvari2010algorithms}.

Benefit from the rapid development of DL technology, it is continuously refreshing the state-of-the-art in the RL field. There is also some survey cover the developments in DRL field, such as \citep{arulkumaran2017brief}. There is also some paper survey the DRL application in NLP filed, such as \citep{ranzato2015sequence} and \citep{bahdanau2016actor}. For robot applications, we can refer to [survey on robot].

%
\section{Traditional Value-based RL Algorithms}
\label{sec:Traditional Value-based RL Algorithms}
The value-based RL algorithms will define the policy $\pi$, by constructing a function of the state value or state -action value.  $Q$-learning algorithm \citep{watkins1992q} is the classical state-action value-based RL algorithm . We can find some traditional ways to improve the $Q$-learning by parameterized function approximator \citep{gordon1996stable}.

%Before we dive into the $Q$-learning, we should consider the Sarsa algorithm at first. We should consider Sarsa as an on-policy algorithm and consider $Q$-learning as an off-policy algorithm. 

%\subsection{Sarsa Algorithm}
%Unlike $Q$-learning algorithm to learn state value, the Sarsa algorithm is to learn the action-value function, $q_\pi (s,a)$. The reward will generated by $q_pi (s_t,a_t)$ and lead to new state-action pair $(s_{t+1},a_{t+1})$, so that we get the chain according to the state, action and reward. The detail of Sarsa algorithm can refer to \citep{sutton2018reinforcement}

\subsection{$Q$-learning Algorithm}
The simplest version of $Q$-learning is the tabular case, which look up the $Q$ value table during the training process.  The  $Q$-learning algorithm applies the Bellman equation to update the Q-value function \citep{bellman1962dynamic}. The ideal is straightforward enough, but it works not very well in the high-dimensional state-action space. The parameterized value function  $Q(s, a; \theta)$ must be used to fit the value. The detail of the basic $Q$-learning algorithm can refer to \citep{sutton2018reinforcement}.

\subsection{Fitted $Q$-learning}
As we could get a lot of experience, in the form of  $(s,a,r,s')$. In fitted $Q$-learning \citep{gordon1996stable}, it use an function approximator to update the  $Q(s, a; \theta_k )$ in each iteration, where  $\theta_k$ with respect to the parameter of  the function  in $k_{th}$ iteration. So we could obtain the target value as the equation below:
$$
\begin{aligned}
y_k = r +\gamma \operatorname*{max}_{a' \in  A} Q(s',a';\theta_{k})
\end{aligned}
$$

\subsection{Neural Fitted $Q$-learning (NFQ)}
The neural fitted $Q$-learning (NFQ) \citep{riedmiller2005neural}, The neural  $Q$-network  receive the state information as the input and  output the probability of each action. The network is parameterized with a neural network $Q(s,a;\theta_k)$ and apply SGD to minimize the MSE. 

But the problem is that the target will be changed as the weights change and generate errors in the state-action space.  We cannot guarantee the convergence by using this algorithm \citep{baird1995residual, tsitsiklis1997analysis, gordon1996stable, riedmiller2005neural}. The other problem is the overestimate issue \citep{van2016deep}. The overestimate come from the max operator when we estimate the value. The max operator will enlarge the noise during the training process.

%
\section{From DQN to Rainbow}
\label{sec:From DQN to Rainbow}

Here we specifically survey the deep $Q$-network (DQN) algorithm \citep{mnih2015human}. The DQN algorithm is the first algorithm that can achieve a superhuman level in the Arcade Learning Environment (ALE) \citep{bellemare2013arcade}. We will also survey various improvements that make up the Rainbow algorithm.

\subsection{Deep $Q$-networks}

There is some common sense as NFQ, and the DQN algorithm is so attractive due to the excellent performance in the variety of ATARI games. It directly learns the raw pixels from the video and keeps the same structure and parameters in all games.  This is unimaginable in previous algorithm implementations, which face the curve of dimension problem in high dimension space. 
To address the unstable and divergence problem, \citep{mnih2015human} have proposed two methods, experience replay, and target $Q$-network.



\begin{enumerate}[(a)]
	\item Experience replay mechanism: DQN initializes a replay buffer $\mathbf{D}$ with transitions $(s_t, a_t, r_t, s_{t+1})$ and randomly choose samples from the buffer to train the deep neural network. The simple idea is very efficient during the implementation.
	
	\item Fixed target $Q$-network: To address the shift of the $Q$-value problem, the target $Q$-network will periodically synchronize parameters with the main $Q$-networks. The performance of DQN seems to be more stable for this technology.
	
\end{enumerate}


\subsection{Double DQN}

To address the over-estimations  issue, \citep{van2016deep} propose a solution to divide the $max$ operation into two parts, action selection part and action evaluation part.  Two networks evaluate and choose action values according to the loss function as below:
$$
\begin{aligned}
Loss=\Big[ r_t + \gamma {Q}_{evaluate} \Big(s_{t+1}, \arg \underset{a_{t+1}}{\max} {Q}_{estimate} \big(s_{t+1}, a_{t+1};{\theta}_{estimate} \big); {\theta}_{evaluate} \Big) - {Q}_{evaluate}(s_t,a_t;{\theta}_{estimate}) \Big]^2
\end{aligned}
$$

The estimate network is parameterized by ${\theta}_{evaluate}$, and apply $\epsilon-greedy$ policy. The evaluate will be the policy of the estimate network and parameterized by ${\theta}_{evaluate}$. The evaluate network will periodically be synchronized with an estimate network.

\subsection{Dueling DQN}

We can decompose the Q-value function into value function and advantage function. So, the dueling DQN used a specialized dueling network architecture to implement this simple idea. \citep{wang2015dueling} proposed to change the single output network to a double output network. They share the same encoder. The formulation can be considered as below:
$$
\begin{aligned}
{Q} (s,a;{\theta_{1}},{\theta_{2}}) =  {V}(s;{\theta_{1}}) + \Big({A} (s,a;{\theta_{1}}) - \frac{\sum_{a'} {A} (s,a';{\theta_{1}})}{|{A}|}		\Big) ,
\end{aligned}
$$
where $\theta_1$ is the parameter of advantage function structure, and $\theta_1$ is the parameter of the value function structure. 


\subsection{Prioritized Replay}

In the DQN algorithm, it proposes to use replay buffer technology and the algorithm uniformly sample from the buffer to train the neural network. \citep{schaul2015prioritized} have a one more step ideal for this technology, the proposed prioritized replay buffer. They set the priority of the experience in the buffer according to the defined rules, and then sample the experience by the priority. The high priority experience is worse to learn. Priority rules are usually set using TD error, but the choice of this method could be defined according to the particular problem. So, it is not easy when we could not know what experience is essential and what is not.

\subsection{Asynchronous Multi-step DQN}
The previous DQN algorithm needs to consider using off-policy algorithms to improve efficiency, and it uses the replay buffer, which occupied much memory and computation resources. \citep{mnih2016asynchronous} proposed an idea to use many agents to train the deep neural network. It applies the asynchronous gradient descent method to train the multiple agents. Each subprocess maintains its own environment, and all the accumulated gradient will be applied to the center.
The n-step methods \citep{sutton2018reinforcement} will be used to update the reward function, and it will be used to trade off the bias and variance problem.

\subsection{Distributional  DQN}
Normally, bellman equation is used to calculate the expected reward value. But \citep{bellemare2017distributional} introduces a different solution that uses the distributional perspective to update the Q-value function. Their motivation is to address the issue caused by the uncertainty of the system, which has multimodal distribution in action space.

In their work, they let  ${Z}(s,a)$ be the return obtained by the current policy. ${Z}$ is not a scalar but a distribution. Then we derived the distributional version of Bellman equation as follows: ${Z}(s,a) = r + \gamma {Z}(s',a')$. 

This advanced proposal has been well used in practice \citep{bellemare2017distributional}, \citep{dabney2018distributional},  \citep{rowland2018analysis}. This method could lead to risk-aware behavior \citep{morimura2010nonparametric} and leads to more robust learning in practice. The reason could be that the distributional perspective maybe provides more training signals than the previous method.

\subsection{Noisy DQN}
\citep{fortunato2017noisy} proposal the Noisy Net. The noisy network could have the same structure as before, but its bias and weights could be perturbed iteratively. The motivations are to solve the limitations of exploring using $\epsilon$-greedy policies. It usually will combine the deterministic and Gaussian noisy stream.
In the implementation status, the randomized action-value function replaces the $\epsilon$-greedy policy. Then, the MLP layers will be parameterized by the noisy network. 

\subsection{Rainbow DQN}

\citep{matteo2017rainbow} propose a solution that integrates all advantages of the seven methods, include DQN, double DQN, prioritized DDQN, dueling DDQN, multi-step, distributed DQN, and noisy DQN. Due to the ensemble of the seven methods, it called the Rainbow DQN.  The combinations lead to a mighty learning algorithm, which is more stable, robust, and efficiency than any single method.
%
\section{The Ohter value-based DRL alogrithms}
\label{sec:The Ohter value-based DRL alogrithms}
\subsection{Deep Q-learning from Demonstrations(DQfD)}
The motivation of the DQfD algorithm \citep{1704.03732} is to address the problem that a large amount of data is required during the training of the DQN algorithm. We have to face the fact that we cannot get enough data from the reality application. This issue also severely restricts the boundary of the deep reinforcement learning. To take full advantage of the non-real-time data, which is generated from the demonstration of people or offline machine, The DQfD algorithm is proposed to accelerate the learning process by only a small amount of demonstration data. The core ideal of the DQfD algorithm is:
\begin{enumerate}[(a)]
	\item Collect the demonstration data and add to replay buffer.
	\item Train the network with the demonstration data before the interaction process.
	\item It combine 1-step TD loss, L2 regularization loss, supervised loss and n-step TD loss.
\end{enumerate}

\subsection{Sym DQN Algorithm}
\citep{1706.02999} proposed the Sym DQN algorithm to exploit symmetries 

%
\bibliography{ValueBasedDRLSurvey}
%\input{test.bbl}
\bibliographystyle{iclr2018_conference}
\newpage
\appendix
 
%
\end{document}

