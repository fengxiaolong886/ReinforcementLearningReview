\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al., 2017]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, O.~P., and Zaremba, W. (2017).
\newblock Hindsight experience replay.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5048--5058.

\bibitem[Arulkumaran et~al., 2017]{arulkumaran2017brief}
Arulkumaran, K., Deisenroth, M.~P., Brundage, M., and Bharath, A.~A. (2017).
\newblock A brief survey of deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1708.05866}.

\bibitem[Bahdanau et~al., 2016]{bahdanau2016actor}
Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville,
  A., and Bengio, Y. (2016).
\newblock An actor-critic algorithm for sequence prediction.
\newblock {\em arXiv preprint arXiv:1607.07086}.

\bibitem[Baird, 1995]{baird1995residual}
Baird, L. (1995).
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In {\em Machine Learning Proceedings 1995}, pages 30--37. Elsevier.

\bibitem[Bellemare et~al., 2017]{bellemare2017distributional}
Bellemare, M.~G., Dabney, W., and Munos, R. (2017).
\newblock A distributional perspective on reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 449--458. JMLR. org.

\bibitem[Bellemare et~al., 2013]{bellemare2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M. (2013).
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47:253--279.

\bibitem[Bellman and Kalaba, 1962]{bellman1962dynamic}
Bellman, R. and Kalaba, R. (1962).
\newblock Dynamic programming applied to control processes governed by general
  functional equations.
\newblock {\em Proceedings of the National Academy of Sciences of the United
  States of America}, 48(10):1735.

\bibitem[Dabney et~al., 2018]{dabney2018distributional}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R. (2018).
\newblock Distributional reinforcement learning with quantile regression.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence}.

\bibitem[Deng et~al., 2016]{deng2016deep}
Deng, Y., Bao, F., Kong, Y., Ren, Z., and Dai, Q. (2016).
\newblock Deep direct reinforcement learning for financial signal
  representation and trading.
\newblock {\em IEEE transactions on neural networks and learning systems},
  28(3):653--664.

\bibitem[Fortunato et~al., 2017]{fortunato2017noisy}
Fortunato, M., Azar, M.~G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
  V., Munos, R., Hassabis, D., Pietquin, O., et~al. (2017).
\newblock Noisy networks for exploration.
\newblock {\em arXiv preprint arXiv:1706.10295}.

\bibitem[Fujimoto et~al., 2018]{fujimoto2018addressing}
Fujimoto, S., van Hoof, H., and Meger, D. (2018).
\newblock Addressing function approximation error in actor-critic methods.
\newblock {\em arXiv preprint arXiv:1802.09477}.

\bibitem[Gandhi et~al., 2017]{gandhi2017learning}
Gandhi, D., Pinto, L., and Gupta, A. (2017).
\newblock Learning to fly by crashing.
\newblock In {\em 2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 3948--3955. IEEE.

\bibitem[Goodfellow et~al., 2016]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A. (2016).
\newblock {\em Deep learning}.
\newblock MIT press.

\bibitem[Gordon, 1996]{gordon1996stable}
Gordon, G.~J. (1996).
\newblock Stable fitted reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1052--1058.

\bibitem[Gordon, 1999]{gordon1999approximate}
Gordon, G.~J. (1999).
\newblock Approximate solutions to markov decision processes.

\bibitem[Gosavi, 2009]{gosavi2009reinforcement}
Gosavi, A. (2009).
\newblock Reinforcement learning: A tutorial survey and recent advances.
\newblock {\em INFORMS Journal on Computing}, 21(2):178--192.

\bibitem[Gu et~al., 2017]{gu2017deep}
Gu, S., Holly, E., Lillicrap, T., and Levine, S. (2017).
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In {\em 2017 IEEE international conference on robotics and automation
  (ICRA)}, pages 3389--3396. IEEE.

\bibitem[Haarnoja et~al., 2018]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}.

\bibitem[Kaelbling et~al., 1996]{kaelbling1996reinforcement}
Kaelbling, L.~P., Littman, M.~L., and Moore, A.~W. (1996).
\newblock Reinforcement learning: A survey.
\newblock {\em Journal of artificial intelligence research}, 4:237--285.

\bibitem[LeCun et~al., 2015]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G. (2015).
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436--444.

\bibitem[Lillicrap et~al., 2015]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D. (2015).
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}.

\bibitem[Matteo et~al., 2017]{matteo2017rainbow}
Matteo, H., Joseph, M., Hado, H., Tom, S., Georg, O., Will, D., Dan, H., Bilal,
  P., Mohammad, A., and David, S. (2017).
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock {\em arXiv: 1710.02298 v1 [cs. AI]}.

\bibitem[Mnih et~al., 2016]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K. (2016).
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937.

\bibitem[Mnih et~al., 2013]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M. (2013).
\newblock Playing atari with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1312.5602}.

\bibitem[Mnih et~al., 2015]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
  (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529.

\bibitem[Morimura et~al., 2010]{morimura2010nonparametric}
Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T. (2010).
\newblock Nonparametric return distribution approximation for reinforcement
  learning.
\newblock In {\em Proceedings of the 27th International Conference on Machine
  Learning (ICML-10)}, pages 799--806.

\bibitem[Pan et~al., 2017]{pan2017virtual}
Pan, X., You, Y., Wang, Z., and Lu, C. (2017).
\newblock Virtual to real reinforcement learning for autonomous driving.
\newblock {\em arXiv preprint arXiv:1704.03952}.

\bibitem[Pinto et~al., 2017]{pinto2017asymmetric}
Pinto, L., Andrychowicz, M., Welinder, P., Zaremba, W., and Abbeel, P. (2017).
\newblock Asymmetric actor critic for image-based robot learning.
\newblock {\em arXiv preprint arXiv:1710.06542}.

\bibitem[Puterman, 2014]{puterman2014markov}
Puterman, M.~L. (2014).
\newblock {\em Markov Decision Processes.: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons.

\bibitem[Ranzato et~al., 2015]{ranzato2015sequence}
Ranzato, M., Chopra, S., Auli, M., and Zaremba, W. (2015).
\newblock Sequence level training with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1511.06732}.

\bibitem[Riedmiller, 2005]{riedmiller2005neural}
Riedmiller, M. (2005).
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In {\em European Conference on Machine Learning}, pages 317--328.
  Springer.

\bibitem[Rowland et~al., 2018]{rowland2018analysis}
Rowland, M., Bellemare, M.~G., Dabney, W., Munos, R., and Teh, Y.~W. (2018).
\newblock An analysis of categorical distributional reinforcement learning.
\newblock {\em arXiv preprint arXiv:1802.08163}.

\bibitem[Schaul et~al., 2015]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015).
\newblock Prioritized experience replay.
\newblock {\em arXiv preprint arXiv:1511.05952}.

\bibitem[Schmidhuber, 2015]{schmidhuber2015deep}
Schmidhuber, J. (2015).
\newblock Deep learning in neural networks: An overview.
\newblock {\em Neural networks}, 61:85--117.

\bibitem[Schulman et~al., 2015]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015).
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages
  1889--1897.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Silver et~al., 2016]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al. (2016).
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484.

\bibitem[Silver et~al., 2018]{silver2018general}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al. (2018).
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock {\em Science}, 362(6419):1140--1144.

\bibitem[Sutton and Barto, 2018]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[Sutton et~al., 2000]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063.

\bibitem[Szepesv{\'a}ri, 2010]{szepesvari2010algorithms}
Szepesv{\'a}ri, C. (2010).
\newblock Algorithms for reinforcement learning.
\newblock {\em Synthesis lectures on artificial intelligence and machine
  learning}, 4(1):1--103.

\bibitem[Tsitsiklis and Van~Roy, 1997]{tsitsiklis1997analysis}
Tsitsiklis, J.~N. and Van~Roy, B. (1997).
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1075--1081.

\bibitem[Van~Hasselt et~al., 2016]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D. (2016).
\newblock Deep reinforcement learning with double q-learning.
\newblock In {\em Thirtieth AAAI conference on artificial intelligence}.

\bibitem[Vinyals et~al., 2019]{vinyals2019alphastar}
Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czarnecki,
  W.~M., Dudzik, A., Huang, A., Georgiev, P., Powell, R., et~al. (2019).
\newblock Alphastar: Mastering the real-time strategy game starcraft ii.
\newblock {\em DeepMind Blog}.

\bibitem[Wang et~al., 2015]{wang2015dueling}
Wang, Z., Schaul, T., Hessel, M., Van~Hasselt, H., Lanctot, M., and De~Freitas,
  N. (2015).
\newblock Dueling network architectures for deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1511.06581}.

\bibitem[Watkins and Dayan, 1992]{watkins1992q}
Watkins, C.~J. and Dayan, P. (1992).
\newblock Q-learning.
\newblock {\em Machine learning}, 8(3-4):279--292.

\end{thebibliography}
