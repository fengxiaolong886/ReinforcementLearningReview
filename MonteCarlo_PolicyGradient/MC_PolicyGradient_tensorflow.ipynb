{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code list implement the Monte Carlo Policy Gradient Alogrithm.\n",
    "------------------------------------------------------------------\n",
    "Input:\n",
    "    differentiable policy function $\\pi_{\\theta}(a|s)$\n",
    "\n",
    "Initalize:\n",
    "    Parameter $\\theta$ for policy function\n",
    "\n",
    "Repeat  experience trajectory:\n",
    "    Use $\\pi_{\\theta}(a|s)$ to generate one trajectory $(s_0,a_0,r_1....s_T)$\n",
    "    Repeat each step in trajectory:\n",
    "        G <--- cumlated reward at time step t\n",
    "        Calculate the policy gradient  $\\Delta\\theta_t = \\alpha \\Delta_{\\theta}log\\pi_{\\theta}(s_t, a_t)G_t$\n",
    "------------------------------------------------------------------\n",
    "\"\"\"\n",
    "import time\n",
    "import pandas as pd\n",
    "import gym\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False\n",
    "LEARNING_RATE = 0.01\n",
    "REWARD_DECAY = 0.95\n",
    "OUTPUT_GRAPH = False\n",
    "ENVNAME = \"MountainCar-v0\"\n",
    "N_LAYER1 = 10\n",
    "N_LAYER2 = 10\n",
    "NUM_EPISODES = 1000\n",
    "ACTIVATION_FUNCTION = tf.nn.tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPG():\n",
    "    def __init__(self, \n",
    "                 n_action, \n",
    "                 n_feature, \n",
    "                 learning_rate=0.01, \n",
    "                 reward_decay=0.95, \n",
    "                 ouput_graph=False,\n",
    "                ):\n",
    "        self.n_action = n_action\n",
    "        self.n_features = n_feature\n",
    "        self.gamma = reward_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.episode_observation = []\n",
    "        self.episode_actions = [] \n",
    "        self.episode_rewards = []\n",
    "        \n",
    "        self.__build_network()\n",
    "        self.sess = tf.Session()\n",
    "        if ouput_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            # http://0.0.0.0:6006/\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)            \n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def __build_network(self):\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            # input\n",
    "            self.tf_observations = tf.placeholder(tf.float32, \n",
    "                                                  [None, self.n_features],\n",
    "                                                  name='observation') \n",
    "            self.tf_acts = tf.placeholder(tf.int32,\n",
    "                                          [None,],\n",
    "                                          name='action_number')\n",
    "            self.tf_actionvalue = tf.placeholder(tf.float32,\n",
    "                                                [None,],\n",
    "                                                name = \"action_value\")\n",
    "        l1 = tf.layers.dense(\n",
    "            inputs=self.tf_observations,\n",
    "            units=N_LAYER1,\n",
    "            activation= ACTIVATION_FUNCTION,\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(seed=1),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(seed=1),\n",
    "            name=\"layer1\"\n",
    "        )\n",
    "        \n",
    "        l2 = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=N_LAYER2,\n",
    "            activation= ACTIVATION_FUNCTION,\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(seed=1),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(seed=1),\n",
    "            name=\"layer2\"\n",
    "        )\n",
    "        \n",
    "        all_action_value = tf.layers.dense(\n",
    "            inputs=l2,\n",
    "            units=self.n_action,\n",
    "            activation= None,\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(seed=1),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(seed=1),\n",
    "            name=\"layer3\"\n",
    "        )\n",
    "        \n",
    "        self.all_action_prob = tf.nn.softmax(all_action_value, name=\"action_prob\")\n",
    "                \n",
    "        with tf.name_scope('loss'):\n",
    "            #  to maximize total reward  (log_p * R) is \n",
    "            #  to minimize -(log_p * R), and the tensorflow only have minimize(loss)\n",
    "            # sparse_softmax_cross_entropy_with_logits is negative log of chosen action\n",
    "            # or in this way:\n",
    "            # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)\\\n",
    "            #                *tf.one_hot(self.tf_acts, self.n_actions), axis=1)\n",
    "            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=all_action_value, labels=self.tf_acts)\n",
    "            # calcuate the loss\n",
    "            loss = tf.reduce_mean(neg_log_prob*self.tf_actionvalue)\n",
    "        with tf.name_scope('train'):\n",
    "            self._train_op = tf.train.AdadeltaOptimizer(self.learning_rate).minimize(loss)    \n",
    "                        \n",
    "    def choose_action(self, observation):\n",
    "        feed_state = observation[np.newaxis,:]\n",
    "        prob_weights = self.sess.run(self.all_action_prob,\n",
    "                                    feed_dict={self.tf_observations:feed_state})\n",
    "        action = np.random.choice(range(prob_weights.shape[1]),\n",
    "                                 p=prob_weights.ravel())\n",
    "        return action\n",
    "    \n",
    "    def store_transistion(self, s, a, r):\n",
    "        self.episode_observation.append(s)\n",
    "        self.episode_actions.append(a)\n",
    "        self.episode_rewards.append(r)\n",
    "\n",
    "    def learn(self):\n",
    "        #discount and normalize the episode reward\n",
    "        discounted_episode_reward_normalized = self._discount_and_norm_rewards()\n",
    "        \n",
    "        # train\n",
    "        self.sess.run(self._train_op,\n",
    "                     feed_dict={\n",
    "                         self.tf_observations : np.vstack(self.episode_observation),\n",
    "                         self.tf_acts : np.array(self.episode_actions),\n",
    "                         self.tf_actionvalue: discounted_episode_reward_normalized,\n",
    "                     })\n",
    "        self.episode_observation = []\n",
    "        self.episode_actions = [] \n",
    "        self.episode_rewards = []\n",
    "        return discounted_episode_reward_normalized\n",
    "    \n",
    "    def _discount_and_norm_rewards(self):\n",
    "        discounted_episode_reward = np.zeros_like(self.episode_rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(self.episode_rewards))):\n",
    "            running_add = running_add * self.gamma + self.episode_rewards[t]\n",
    "            discounted_episode_reward[t] = running_add\n",
    "        \n",
    "        # normalized\n",
    "        discounted_episode_reward -= np.mean(discounted_episode_reward)\n",
    "        discounted_episode_reward /= np.std(discounted_episode_reward)\n",
    "        return discounted_episode_reward        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_stats1(rec, xlabel, ylabel,title):\n",
    "    fig = plt.figure(figsize=(20, 10), facecolor = \"white\")\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(rec) \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    return fig\n",
    "\n",
    "def plot_episode_stats2(stats):\n",
    "    # Plot time steps and episode number\n",
    "    fig3 = plt.figure(figsize=(20, 10))\n",
    "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Episode\")\n",
    "    plt.title(\"Episode per time step\")\n",
    "    return fig3\n",
    "\n",
    "def Plot_the_result(rec):\n",
    "    # Plot episode length over time\n",
    "    episode_lengths = rec.episode_lengths\n",
    "    fig = plot_episode_stats(episode_lengths, \n",
    "                       xlabel = \"Episode\",\n",
    "                       ylabel = \"Episode Length\",\n",
    "                       title = \"Episode length over Time\"\n",
    "            )\n",
    "    fig.show()\n",
    "#    fig.savefig(\"./log/FA_QLearning_MountainCar_EpisodeLength.jpg\")\n",
    "\n",
    "    # Plot Episode reward over time\n",
    "    smoohing_window = 10\n",
    "    reward_smooths = pd.Series(rec.episode_rewards).rolling(smoohing_window,\\\n",
    "                    min_periods = smoohing_window).mean()\n",
    "    fig = plot_episode_stats1(reward_smooths, \n",
    "                       xlabel = \"Episode\",\n",
    "                       ylabel = \"Episode Reward\",\n",
    "                       title = \"Episode reward over time\"\n",
    "            )\n",
    "    fig.show()\n",
    "#    fig.savefig(\"./log/FA_QLearning_Mountain_EpisodeReward.jpg\")\n",
    "    \n",
    "    # Plot Episode per time step\n",
    "    fig = plot_episode_stats2(rec)\n",
    "    fig.show()\n",
    "#    fig.savefig(\"./log/FA_QLearning_Mountain_EpisodePerTimeStep.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(RL, env, num_episodes):\n",
    "    # Track the statistics of the result\n",
    "    record = namedtuple(\"Record\", [\"episode_lengths\",\"episode_rewards\"])\n",
    "    \n",
    "    rec = record(episode_lengths=np.zeros(num_episodes),\n",
    "                          episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        if 0 == (i_episode +1) % 1:\n",
    "            print(\"This the episode {}/{}\".format(i_episode, num_episodes), end = \"\\r\")\n",
    "        observation = env.reset()\n",
    "        step =0\n",
    "        reward_cum = 0\n",
    "        done = False\n",
    "        while True:\n",
    "            #env.render()\n",
    "            if RENDER_ENV:\n",
    "                env.render()\n",
    "            # step1: choose action based on the state\n",
    "            action = RL.choose_action(observation)\n",
    "            # step2: take the action in the enviroment\n",
    "            observation_next, reward, done, info = env.step(action)\n",
    "            # step3: store the transistion for training\n",
    "            RL.store_transistion(observation, action, reward)\n",
    "            # update the record\n",
    "            step += 1\n",
    "            rec.episode_lengths[i_episode] = step \n",
    "            rec.episode_rewards[i_episode] += reward\n",
    "            if done:\n",
    "                # step4: train the network\n",
    "                RL.learn()\n",
    "                print(\"The reward at episode {} is {}.\".format(i_episode, \n",
    "                                                              rec.episode_rewards[i_episode]))\n",
    "                break\n",
    "            # step5: save the new state\n",
    "            observation = observation_next\n",
    "    print(\"Finished\")\n",
    "    env.close()\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-44bc6c13e505>:45: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f13b5856cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "The reward at episode 0 is -492483.0.\n",
      "The reward at episode 1 is -2439122.0.\n",
      "The reward at episode 2 is -2868666.0.\n",
      "The reward at episode 3 is -201182.0.\n",
      "The reward at episode 4 is -1163759.0.\n",
      "The reward at episode 5 is -27378.0.\n",
      "The reward at episode 6 is -1187720.0.\n",
      "The reward at episode 7 is -1142153.0.\n",
      "The reward at episode 8 is -659508.0.\n",
      "The reward at episode 9 is -1496455.0.\n",
      "The reward at episode 10 is -1641807.0.\n",
      "The reward at episode 11 is -656461.0.\n",
      "The reward at episode 12 is -2558302.0.\n",
      "The reward at episode 13 is -70324.0.\n",
      "The reward at episode 14 is -221932.0.\n",
      "The reward at episode 15 is -1339231.0.\n",
      "The reward at episode 16 is -827406.0.\n",
      "The reward at episode 17 is -47209.0.\n",
      "The reward at episode 18 is -706158.0.\n",
      "The reward at episode 19 is -455305.0.\n",
      "The reward at episode 20 is -1575169.0.\n",
      "The reward at episode 21 is -736898.0.\n",
      "This the episode 22/1000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-03d615fda6b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m              \u001b[0mreward_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREWARD_DECAY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m              ouput_graph=OUTPUT_GRAPH)\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#Plot the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mPlot_the_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6a041aec286e>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(RL, env, num_episodes)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# step1: choose action based on the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m# step2: take the action in the enviroment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mobservation_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-44bc6c13e505>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                     feed_dict={self.tf_observations:feed_state})\n\u001b[1;32m     86\u001b[0m         action = np.random.choice(range(prob_weights.shape[1]),\n\u001b[0;32m---> 87\u001b[0;31m                                  p=prob_weights.ravel())\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENVNAME)\n",
    "    env = env.unwrapped\n",
    "    RL = MCPG(n_action=env.action_space.n,\n",
    "             n_feature=env.observation_space.shape[0],\n",
    "             learning_rate=LEARNING_RATE,\n",
    "             reward_decay=REWARD_DECAY,\n",
    "             ouput_graph=OUTPUT_GRAPH)\n",
    "    rec = update(RL, env, num_episodes=NUM_EPISODES)\n",
    "    #Plot the result\n",
    "    Plot_the_result(rec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
