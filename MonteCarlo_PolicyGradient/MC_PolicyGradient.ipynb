{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code list implement the Monte Carlo Policy Gradient Alogrithm.\n",
    "------------------------------------------------------------------\n",
    "Input:\n",
    "    differentiable policy function $\\pi_{\\theta}(a|s)$\n",
    "\n",
    "Initalize:\n",
    "    Parameter $\\theta$ for policy function\n",
    "\n",
    "Repeat  experience trajectory:\n",
    "    Use $\\pi_{\\theta}(a|s)$ to generate one trajectory $(s_0,a_0,r_1....s_T)$\n",
    "    Repeat each step in trajectory:\n",
    "        G <--- cumlated reward at time step t\n",
    "        Calculate the policy gradient  $\\Delta\\theta_t = \\alpha \\Delta_{\\theta}log\\pi_{\\theta}(s_t, a_t)G_t$\n",
    "------------------------------------------------------------------\n",
    "\"\"\"\n",
    "import gym\n",
    "import os \n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.action_sapce: 2\n",
      "env.observation_sapce: 4\n",
      "env.observation_space.high: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "env.observation_space.low: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "print(\"env.action_sapce:\", env.action_space.n)\n",
    "print(\"env.observation_sapce:\", env.observation_space.shape[0])\n",
    "print(\"env.observation_space.high:\", env.observation_space.high)\n",
    "print(\"env.observation_space.low:\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPG():\n",
    "    def __init__(self, env, estimator, actions, discount=1.0, alpha=0.5, epsilon=0.1):\n",
    "        self.actions = actions\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.n_features = env.observation_space.n\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.epsisode_states = []\n",
    "        self.episode_actions = [] \n",
    "        self.episode_rewards = []\n",
    "        self.__build_network()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def __build_network():\n",
    "        # input\n",
    "        self.tf_observations = tf.placeholder(tf.float32, [None, self.n_features], name='X') \n",
    "        # for calculating loss\n",
    "        self.tf_acts = tf.placeholder(tf.float32, [None, self.n_actions], name='Y')\n",
    "        \n",
    "        with tf.variable_scope('PolicyNetwork'):\n",
    "            # c_names(collections_names) are the collections to store variables\n",
    "            c_names = ['net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            n_l1 = 10\n",
    "            n_l2 = 10\n",
    "            w_initializer = tf.random_normal_initializer(0., 0.3)\n",
    "            b_initializer = tf.constant_initializer(0.1)\n",
    "            \n",
    "            # first layer. \n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], \n",
    "                                     initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], \n",
    "                                     initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "\n",
    "            # second layer.\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, n_l2], \n",
    "                                     initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, n_l2], \n",
    "                                     initializer=b_initializer, collections=c_names)\n",
    "                l2 = tf.nn.relu(tf.matmul(l1, w2) + b2)\n",
    "                \n",
    "            # third layer.\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], \n",
    "                                     initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], \n",
    "                                     initializer=b_initializer, collections=c_names)\n",
    "                all_action = tf.matmul(l1, w2) + b2\n",
    "                self.all_action_prob = tf.nn.softmax(all_action)\n",
    "                \n",
    "        with tf.variable_scope('loss'):\n",
    "            #  to maximize total reward  (log_p * R) is \n",
    "            #  to minimize -(log_p * R), and the tensorflow only have minimize(loss)\n",
    "            # sparse_softmax_cross_entropy_with_logits is negative log of chosen action\n",
    "            # or in this way:\n",
    "            # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)\\\n",
    "            #                *tf.one_hot(self.tf_acts, self.n_actions), axis=1)\n",
    "            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_action,\n",
    "                                                                    labels=self.tf_acts)\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)    \n",
    "            \n",
    "            \n",
    "    def choose_action(self, observation):\n",
    "        # select action\n",
    "        if np.random.uniform() > self.epsilon:\n",
    "            # choose the best action\n",
    "            state_action = self.estimator.predict(observation)\n",
    "            action = np.argmax(state_action)\n",
    "        else:\n",
    "            # choose a random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        return action\n",
    "    \n",
    "    def store_transistion(self, s, a, r):\n",
    "        pass\n",
    "\n",
    "    def learn(self, s, a, r, s_,done):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(RL, env, num_episodes):\n",
    "    # Track the statistics of the result\n",
    "    record = namedtuple(\"Record\", [\"episode_lengths\",\"episode_rewards\"])\n",
    "    \n",
    "    rec = record(episode_lengths=np.zeros(num_episodes),\n",
    "                          episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        if 0 == (i_episode +1) % 2:\n",
    "            print(\"This the episode {}/{}\".format(i_episode, num_episodes), end = \"\\r\")\n",
    "        observation = env.reset()\n",
    "        step =0\n",
    "        reward = 0\n",
    "        while True:\n",
    "            #env.render()\n",
    "            if RENDER_ENV:\n",
    "                env.render()\n",
    "            # step1: choose action based on the state\n",
    "            action = RL.choose_action(observation)\n",
    "            \n",
    "            # step2: take the action in the enviroment\n",
    "            observation_next, reward, done, info = env.step(action)\n",
    "            \n",
    "            # step3: store the transistion for training\n",
    "            RL.store_transistion(observation, action, reward)\n",
    "            # update the record\n",
    "            step += 1\n",
    "            rec.episode_lengths[i_episode] = step \n",
    "            rec.episode_rewards[i_episode] += reward\n",
    "\n",
    "            if done:\n",
    "                # step4: train the network\n",
    "                RL.learn(observation, action, reward, observation_next, done)\n",
    "                break\n",
    "                \n",
    "            # step5: save the new state\n",
    "            observation = observation_next\n",
    "                \n",
    "    print(\"Finished\")\n",
    "    env.close()\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_stats(rec, xlabel, ylabel,title):\n",
    "    fig = plt.figure(figsize=(20, 10), facecolor = \"white\")\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(rec) \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_stats(stats):\n",
    "    # Plot time steps and episode number\n",
    "    fig3 = plt.figure(figsize=(20, 10))\n",
    "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Episode\")\n",
    "    plt.title(\"Episode per time step\")\n",
    "    return fig3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_the_result(rec):\n",
    "    # Plot episode length over time\n",
    "    episode_lengths = rec.episode_lengths\n",
    "    fig = plot_episode_stats(episode_lengths, \n",
    "                       xlabel = \"Episode\",\n",
    "                       ylabel = \"Episode Length\",\n",
    "                       title = \"Episode length over Time\"\n",
    "            )\n",
    "    fig.show()\n",
    "#    fig.savefig(\"./log/FA_QLearning_MountainCar_EpisodeLength.jpg\")\n",
    "\n",
    "    # Plot Episode reward over time\n",
    "    smoohing_window = 10\n",
    "    reward_smooths = pd.Series(rec.episode_rewards).rolling(smoohing_window,\\\n",
    "                    min_periods = smoohing_window).mean()\n",
    "    fig = plot_episode_stats(reward_smooths, \n",
    "                       xlabel = \"Episode\",\n",
    "                       ylabel = \"Episode Reward\",\n",
    "                       title = \"Episode reward over time\"\n",
    "            )\n",
    "    fig.show()\n",
    "#    fig.savefig(\"./log/FA_QLearning_Mountain_EpisodeReward.jpg\")\n",
    "    \n",
    "    # Plot Episode per time step\n",
    "    fig = plot_episode_stats(rec)\n",
    "    fig.show()\n",
    "#    fig.savefig(\"./log/FA_QLearning_Mountain_EpisodePerTimeStep.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    num_episodes = 1000\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    actions = [i for i in range(env.action_space.n)] \n",
    "    RL = MCPG(env, estimator, actions, discount=1.0, alpha=0.5, epsilon=0.1)\n",
    "    rec = update(RL, env, num_episodes=num_episodes)\n",
    "    #Plot the result\n",
    "    Plot_the_result(rec)\n",
    "    \n",
    "    end_time= time.time()\n",
    "    print(\"This alogrithm cost time is :\",end_time-start_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
